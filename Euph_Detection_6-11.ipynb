{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7db7b57-593f-402b-9e89-bd9b91b2ebfc",
   "metadata": {},
   "source": [
    "## Testing on euph corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a826d2-feed-40fa-82bc-b522d3378c67",
   "metadata": {},
   "source": [
    "#### Load and preprocess euph sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139ed7ad-2115-4b72-b97a-f1c37bd661c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6314a02b-81f9-4e16-a30f-c4c1ca917685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/leep6/anaconda3/lib/python3.8/site-packages (1.2.4)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.7 MB 29.0 MB/s eta 0:00:01████▏                    | 4.1 MB 29.0 MB/s eta 0:00:01██████████████████████▏       | 8.8 MB 29.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /home/leep6/anaconda3/lib/python3.8/site-packages (from pandas) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/leep6/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/leep6/anaconda3/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/leep6/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas) (1.15.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.2.4\n",
      "    Uninstalling pandas-1.2.4:\n",
      "      Successfully uninstalled pandas-1.2.4\n",
      "Successfully installed pandas-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf54fb6-1ff6-4b92-932b-a55b2f50d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'(##\\d*\\W)|<\\w>|,|;|:|--|\\(|\\)|#|%|\\\\|\\/|\\.|\\*|\\+|@', '', s)\n",
    "    s = re.sub(r'\\s\\s+', ' ', s)\n",
    "    s = s.lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "346edc29-5cd1-4af3-bfa0-22dc8c95dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess sentences\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    text = euph_corpus.loc[i, 'sentence']\n",
    "    euph_corpus.loc[i, 'sentence'] = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeef0f1e-1224-4a47-88ea-7524a33eb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrasify the sentences\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "\n",
    "bigram_phraser = Phraser.load('bigram_phraser_7')\n",
    "trigram_phraser = Phraser.load('trigram_phraser_7')\n",
    "euph_corpus['phrases'] = \"\"\n",
    "data = [] # holds phrased input sentences to update wv model with\n",
    "\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    text = euph_corpus.loc[i, 'sentence']\n",
    "    euph_corpus.at[i, 'phrases'] = bigram_phraser[text.split()] # use phraser to detect phrases in text\n",
    "    euph_corpus.at[i, 'phrases'] = trigram_phraser[euph_corpus.loc[i, 'phrases']]\n",
    "    data.append(euph_corpus.loc[i, 'phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26467c31-7785-47b9-9e21-694bcbaff4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'we',\n",
       " 'talking',\n",
       " 'the',\n",
       " 'merits',\n",
       " \"of'\",\n",
       " 'enhanced_interrogation_techniques',\n",
       " 'or',\n",
       " 'the',\n",
       " 'definition_of_torture']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm the phraser is still working\n",
    "trigram_phraser[['are', 'we', 'talking', 'the', 'merits', \"of'\", 'enhanced_interrogation', 'techniques', 'or', 'the', 'definition', 'of', 'torture']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e543dfa-0ea0-4c08-9c34-ef8e1ccdaa65",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define topic similarity function, topic list and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35f09267-a883-4fc2-83b8-ef3ad151c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_similarity(phrase, topic_list):\n",
    "    score = 0\n",
    "    for topic in topic_list:\n",
    "        try:\n",
    "            similarity = model.wv.similarity(phrase, topic)\n",
    "            if (similarity > 0):\n",
    "                score += similarity\n",
    "        except:\n",
    "            score += 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d6bd72d-a73a-4123-a655-e459d0e0c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define topic list and stopwords\n",
    "topic_list = ['politics', 'death', 'kill', 'crime',\n",
    "               'drugs', 'alcohol', 'fat', 'old', 'poor', 'cheap',\n",
    "               'sex', 'sexual',\n",
    "               'employment', 'job', 'disability', 'disabled', \n",
    "               'accident', 'pregnant', 'poop', 'sickness', 'race', 'racial', 'vomit'\n",
    "              ]\n",
    "\n",
    "stopwords = []\n",
    "#['the', 'a', 'to', 'him', 'her', 'them', 'me', 'you', 'of', 'with']\n",
    "\n",
    "with open('stopwords.txt','rb') as f:\n",
    "    content = f.read()\n",
    "    content = content.split(b'\\r\\n')\n",
    "    for line in content:\n",
    "        stopwords.append(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57967be1-b4ae-4b11-a476-4b337dd20d98",
   "metadata": {},
   "source": [
    "#### Perform topic filtering and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04c02c89-8dca-4152-b11d-449191d9807b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c9c7e8594c82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/wv_model_7\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# typically takes 45-90 seconds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# train model on input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# define model and train on new data\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"data/wv_model_7\") # typically takes 45-90 seconds\n",
    "# train model on input data \n",
    "model.train(data, total_examples=len(data), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "709d0eb6-daff-429c-9104-93859aa1ac70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained the euphemism in 1626 out of 1965 sentences\n",
      "14787 quality phrases retained overall\n",
      "Filtered 7034 non-keywords out\n",
      "\n",
      "EXACT SUCCESSES: ['tinkle', 'undocumented immigrants', 'undocumented immigrant', 'venereal diseases', 'venereal disease', 'sex workers', 'sex worker', 'mentally disabled', 'correctional facilities', 'correctional facility', 'freedom fighters', 'freedom fighter', 'detainees', 'detainee', 'psychiatric hospital', 'ethnic cleansing', 'ethnically cleansed', 'enhanced interrogation techniques', 'mistruths', 'mistruth', 'elderly', 'armed conflict', 'drinking problem', 'deceased', 'pro-life', 'income inequality', 'rear end', 'lavatory', 'birds and the bees', 'inner city', 'developing country', 'developed country', 'substance abuse', 'global south', 'underprivileged', 'inebriated', 'homemaker', 'capital punishment', 'differently-abled', 'indigent', 'detention camp', 'pass gas', 'dearly departed', 'terminating a pregnancy', 'pregnancy termination', 'senior citizen', 'senior citizens', 'substance abuser', 'substance abusers', 'undocumented workers', 'pre-owned', 'sanitation workers', 'sanitation worker', 'latrine', 'plus-sized', 'physically challenged', 'developmentally disabled', 'custodians', 'same sex', 'able-bodied', 'hearing impaired', 'under the weather', 'people of color', 'adult beverages', 'fatalities', 'fatality', 'pro-choice', 'targeted killings', 'targeted killing', 'low-income', 'less fortunate', 'advanced age', 'mentally challenged', 'droppings', 'portly', 'negative cash flow', 'golden years', 'perished', 'perish', 'passed away', 'passing away', 'pass away', 'neutralize', 'neutralized', 'overweight', 'aging', 'chest', 'demise', 'slim', 'dismissed', 'sober', 'collateral damage', 'deprived', 'plump', 'same-sex', 'stout', 'disabled', 'special needs', 'disadvantaged', 'underdeveloped', 'invalid', 'mixed up', 'economical', 'over the hill', 'intoxicated', 'regime change', 'exterminate', 'custodian', 'put to sleep', 'downsize', 'laid off', 'laying off', 'lay off', 'went to heaven', 'accident', 'gluteus maximus', 'oldest profession', 'late', 'experienced', 'outspoken', 'wealthy', 'troubled', 'seeing someone', 'slept with', 'weed']\n",
      "\n",
      "PARTIAL SUCCESSES: ['undocumented immigrants', 'mentally disabled', 'detainees', 'ethnic cleansing', 'elderly', 'deceased', 'pro-life', 'income inequality', 'rear end', 'inner city', 'substance abuse', 'underprivileged', 'homemaker', 'indigent', 'pre-owned', 'same sex', 'able-bodied', 'fatality', 'fatalities', 'pro-choice', 'low-income', 'less fortunate', 'droppings', 'golden years', 'perished', 'perish', 'neutralize', 'overweight', 'aging', 'chest', 'demise', 'slim', 'dismissed', 'sober', 'collateral damage', 'plump', 'same-sex', 'stout', 'disabled', 'special needs', 'disadvantaged', 'underdeveloped', 'economical', 'intoxicated', 'regime change', 'exterminate', 'expecting', 'lay off', 'laid off', 'laying off', 'accident', 'late', 'outspoken', 'wealthy', 'troubled', 'weed', 'let them go', 'deprived', 'invalid', 'custodian', 'experienced', 'with child']\n",
      "\n",
      "FAILURES: ['undocumented immigrants', 'sex workers', 'mentally disabled', 'correctional facility', 'detainees', 'comfort women', 'ethnic cleansing', 'lost my lunch', 'lose your lunch', 'lose their lunch', 'enhanced interrogation techniques', 'elderly', 'deceased', 'pro-life', 'income inequality', 'rear end', 'inner city', 'developed country', 'developing country', 'substance abuse', 'underprivileged', 'homemaker', 'differently-abled', 'indigent', 'detention camp', 'economical with the truth', 'substance abusers', 'undocumented workers', 'street person', 'full figured', 'same sex', 'able-bodied', 'people of color', 'broken home', 'fatality', 'fatalities', 'pro-choice', 'low-income', 'less fortunate', 'advanced age', 'mentally challenged', 'droppings', 'time of the month', 'made love', 'making love', 'make love', 'running behind', 'let go of', 'let him go', 'letting someone go', 'perished', 'perish', 'passing on', 'pass on', 'neutralize', 'overweight', 'aging', 'chest', 'demise', 'slim', 'dismissed', 'sober', 'collateral damage', 'plump', 'same-sex', 'go all the way', 'stout', 'disabled', 'disadvantaged', 'underdeveloped', 'invalid', 'well off', 'economical', 'intoxicated', 'exterminate', 'a certain age', 'between jobs', 'long sleep', 'getting clean', 'downsize', 'expecting', 'lay off', 'laid off', 'to go to heaven', 'accident', 'outlived her usefulness', 'outlived their usefulness', 'got clean', 'late', 'wealthy', 'troubled', 'seeing someone', 'seasoned', 'sleep with', 'slept with', 'sleep around', 'with child', 'weed', 'let them go', 'let us go', 'let her go', 'pass away', 'regime change', 'custodian', 'outlived his usefulness', 'outspoken', 'seeing each other']\n",
      "\n",
      "FALSE NEGATIVES of TOPIC FILTERING: ['expecting', 'seasoned', 'outlived his usefulness']\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 1.5\n",
    "score = 0\n",
    "\n",
    "successes = []\n",
    "partial_successes = []\n",
    "failures = []\n",
    "topically_filtered_euphs = []\n",
    "quality_phrase_count = 0\n",
    "filtered = []\n",
    "\n",
    "euph_corpus['quality_phrases'] = \"\"\n",
    "\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    text = euph_corpus.loc[i, 'sentence']\n",
    "    phrases = euph_corpus.loc[i, 'phrases']\n",
    "    euph = euph_corpus.loc[i, 'keyword']\n",
    "    quality_phrases = []\n",
    "    for phrase in phrases:\n",
    "        if (phrase in stopwords):\n",
    "            continue\n",
    "        similarity = sum_similarity(phrase, topic_list)\n",
    "        if (similarity > THRESHOLD and phrase not in quality_phrases):\n",
    "            quality_phrases.append(phrase)\n",
    "        elif (similarity < THRESHOLD and euph == re.sub(r'_', ' ', phrase)):\n",
    "            if euph not in topically_filtered_euphs:\n",
    "                topically_filtered_euphs.append(euph)\n",
    "        else:\n",
    "            filtered.append(phrase)\n",
    "    # add the quality phrases to the column\n",
    "    euph_corpus.at[i, 'quality_phrases'] = quality_phrases\n",
    "    \n",
    "    # now check if the euph in the sentence is retained in the list of quality phrases\n",
    "    quality_phrases = [re.sub(r'_', ' ', p) for p in quality_phrases]\n",
    "    quality_phrase_count += len(quality_phrases)\n",
    "    \n",
    "    if euph in quality_phrases:\n",
    "        score += 1\n",
    "        if euph not in successes:\n",
    "            successes.append(euph)\n",
    "    else:\n",
    "        partial_success = False\n",
    "        for p in quality_phrases: # check if phrase output contains euphemism\n",
    "            if euph in p:\n",
    "                score += 1\n",
    "                if euph not in partial_successes:\n",
    "                    partial_successes.append(euph)\n",
    "                    partial_success = True\n",
    "                    break\n",
    "        if (partial_success == False): \n",
    "            if euph not in failures:\n",
    "                failures.append(euph)\n",
    "\n",
    "            # check failures for a particular phrase\n",
    "            # if (euph == \"ethnic cleansing\"):\n",
    "            #     print(\"TEXT: {}\".format(text))\n",
    "            #     print(\"PHRASES: {}\".format(phrases))\n",
    "            #     print(\"QUALITY PHRASES: {}\".format(quality_phrases))\n",
    "            #     print()\n",
    "\n",
    "print(\"Retained the euphemism in {} out of {} sentences\".format(score, len(euph_corpus)))\n",
    "print(\"{} quality phrases retained overall\".format(quality_phrase_count))\n",
    "print(\"Filtered {} non-keywords out\".format(len(filtered)))\n",
    "print()\n",
    "print(\"EXACT SUCCESSES: {}\".format(successes))\n",
    "print()\n",
    "print(\"PARTIAL SUCCESSES: {}\".format(partial_successes))\n",
    "print()\n",
    "print(\"FAILURES: {}\".format(failures))\n",
    "print()\n",
    "print(\"FALSE NEGATIVES of TOPIC FILTERING: {}\".format(topically_filtered_euphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "194d9de7-d23f-4b43-9f77-26f87fa262ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics: 0.11407020688056946\n",
      "death: 0.08639270067214966\n",
      "kill: 0.31013041734695435\n",
      "crime: 0.3427277207374573\n",
      "drugs: 0.5708686113357544\n",
      "alcohol: 0.6496191024780273\n",
      "fat: 0.5302789211273193\n",
      "old: 0.24798649549484253\n",
      "poor: 0.1948791742324829\n",
      "cheap: 0.41431498527526855\n",
      "sex: 0.30003926157951355\n",
      "sexual: 0.15527567267417908\n",
      "employment: -0.014575891196727753\n",
      "job: 0.12978608906269073\n",
      "disability: -0.025316180661320686\n",
      "disabled: -0.03400751203298569\n",
      "accident: 0.17667508125305176\n",
      "pregnant: 0.2841845154762268\n",
      "poop: 0.6066109538078308\n",
      "sickness: 0.3730947971343994\n",
      "race: 0.10498347878456116\n",
      "racial: 0.1140114963054657\n",
      "vomit: 0.6100114583969116\n",
      "SIMILAR TOPICS: ['kill', 'crime', 'drugs', 'alcohol', 'fat', 'old', 'cheap', 'sex', 'pregnant', 'poop', 'sickness', 'vomit']\n",
      "TOTAL SCORE: 6.315941140055656\n"
     ]
    }
   ],
   "source": [
    "# testing - for topic similarity queries on a single phrase\n",
    "test_phrase = 'weed'\n",
    "similar_topics = []\n",
    "score = 0\n",
    "for topic in topic_list:\n",
    "    similarity = model.wv.similarity(test_phrase, topic)\n",
    "    if (similarity > 0.24):\n",
    "        similar_topics.append(topic)\n",
    "    if (similarity > 0):\n",
    "        score += similarity\n",
    "    print('{}: {}'.format(topic, similarity))\n",
    "\n",
    "print('SIMILAR TOPICS: {}'.format(similar_topics))\n",
    "print('TOTAL SCORE: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0c452-ce5d-428d-9a7e-96a2a34158bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02934f-696c-4936-ae36-e6fb67d1ce9d",
   "metadata": {},
   "source": [
    "#### Load up the checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2697674-4209-4dc6-bc72-c6d979492e49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>edited_text</th>\n",
       "      <th>is_euph</th>\n",
       "      <th>category</th>\n",
       "      <th>type</th>\n",
       "      <th>euph_status</th>\n",
       "      <th>sentence</th>\n",
       "      <th>phrases</th>\n",
       "      <th>quality_phrases</th>\n",
       "      <th>keyword_present</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>same sex</td>\n",
       "      <td>Multiple \"partners\" as you call it was NEVER i...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>same sex</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>multiple partners and same sex partners was ne...</td>\n",
       "      <td>['multiple_partners', 'and', 'same_sex', 'part...</td>\n",
       "      <td>['multiple_partners', 'same_sex', 'partners', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>correctional facility</td>\n",
       "      <td>Ray Reyna Jr. said Los Banos Police will aid t...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>correctional facility</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>this was such a tragic case and and booked on ...</td>\n",
       "      <td>['this', 'was', 'such', 'a', 'tragic_case', 'a...</td>\n",
       "      <td>['tragic_case', 'suspicion_of_felony', 'dui', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>detainees</td>\n",
       "      <td>The disclosure that the detainee, Adnan Farhan...</td>\n",
       "      <td>1</td>\n",
       "      <td>politics</td>\n",
       "      <td>detainee</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>instead the detainees remain stuck in legal li...</td>\n",
       "      <td>['instead', 'the', 'detainees', 'remain_stuck'...</td>\n",
       "      <td>['detainees', 'remain_stuck', 'legal_limbo']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>mentally challenged</td>\n",
       "      <td>Society as a whole has the respect for life sl...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>mentally challenged</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>had down syndrome or was crippled mentally cha...</td>\n",
       "      <td>['had', 'down_syndrome', 'or', 'was', 'cripple...</td>\n",
       "      <td>['down_syndrome', 'crippled', 'mentally_challe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>754</th>\n",
       "      <td>golden years</td>\n",
       "      <td>The new figures come out of BC's Center for Re...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>golden years</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>only about 48 percent of current working house...</td>\n",
       "      <td>['only', 'about', '48_percent', 'of', 'current...</td>\n",
       "      <td>['48_percent', 'working_households', 'ready', ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>custodian</td>\n",
       "      <td>Ten dollars can help a kindergartner @ @ @ @ @...</td>\n",
       "      <td>1</td>\n",
       "      <td>employment</td>\n",
       "      <td>custodian</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>his journey has included a stint playing on hi...</td>\n",
       "      <td>['his_journey', 'has', 'included_a_stint', 'pl...</td>\n",
       "      <td>['his_journey', 'included_a_stint', 'playing',...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>deceased</td>\n",
       "      <td>This Must Be the Place-- Sean Penn stars in th...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>deceased</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>this must be the place sean penn stars in this...</td>\n",
       "      <td>['this', 'must_be', 'the', 'place', 'sean_penn...</td>\n",
       "      <td>['sean_penn', 'comedy', 'retired', 'rock_star'...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>collateral damage</td>\n",
       "      <td>Arkin then turns to Afghanistan, arguing that ...</td>\n",
       "      <td>1</td>\n",
       "      <td>death</td>\n",
       "      <td>collateral damage</td>\n",
       "      <td>sometimes_euph</td>\n",
       "      <td>hence we need not be overly concerned about ci...</td>\n",
       "      <td>['hence', 'we_need', 'not', 'be', 'overly_conc...</td>\n",
       "      <td>['overly_concerned', 'collateral_damage', \"'\"]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>rear end</td>\n",
       "      <td>Howard said he might be. Roger said that there...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>rear end</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>roger said that there were people who came on ...</td>\n",
       "      <td>['roger_said', 'that', 'there_were', 'people_w...</td>\n",
       "      <td>['people_who', 'touch', 'rear_end']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>senior citizens</td>\n",
       "      <td>Angie Hiesl certainly knows how to shock and a...</td>\n",
       "      <td>1</td>\n",
       "      <td>physical/mental attributes</td>\n",
       "      <td>senior citizen</td>\n",
       "      <td>always_euph</td>\n",
       "      <td>that's because her work involve placing senior...</td>\n",
       "      <td>[\"that's\", 'because', 'her', 'work', 'involve_...</td>\n",
       "      <td>['involve_placing', 'senior_citizens', 'white_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1382 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    keyword  \\\n",
       "554                same sex   \n",
       "67    correctional facility   \n",
       "97                detainees   \n",
       "713     mentally challenged   \n",
       "754            golden years   \n",
       "...                     ...   \n",
       "1184              custodian   \n",
       "212                deceased   \n",
       "978       collateral damage   \n",
       "278                rear end   \n",
       "473         senior citizens   \n",
       "\n",
       "                                            edited_text  is_euph  \\\n",
       "554   Multiple \"partners\" as you call it was NEVER i...        1   \n",
       "67    Ray Reyna Jr. said Los Banos Police will aid t...        1   \n",
       "97    The disclosure that the detainee, Adnan Farhan...        1   \n",
       "713   Society as a whole has the respect for life sl...        1   \n",
       "754   The new figures come out of BC's Center for Re...        1   \n",
       "...                                                 ...      ...   \n",
       "1184  Ten dollars can help a kindergartner @ @ @ @ @...        1   \n",
       "212   This Must Be the Place-- Sean Penn stars in th...        1   \n",
       "978   Arkin then turns to Afghanistan, arguing that ...        1   \n",
       "278   Howard said he might be. Roger said that there...        1   \n",
       "473   Angie Hiesl certainly knows how to shock and a...        1   \n",
       "\n",
       "                        category                   type     euph_status  \\\n",
       "554   physical/mental attributes               same sex     always_euph   \n",
       "67                    employment  correctional facility     always_euph   \n",
       "97                      politics               detainee     always_euph   \n",
       "713   physical/mental attributes    mentally challenged     always_euph   \n",
       "754   physical/mental attributes           golden years     always_euph   \n",
       "...                          ...                    ...             ...   \n",
       "1184                  employment              custodian  sometimes_euph   \n",
       "212                        death               deceased     always_euph   \n",
       "978                        death      collateral damage  sometimes_euph   \n",
       "278   physical/mental attributes               rear end     always_euph   \n",
       "473   physical/mental attributes         senior citizen     always_euph   \n",
       "\n",
       "                                               sentence  \\\n",
       "554   multiple partners and same sex partners was ne...   \n",
       "67    this was such a tragic case and and booked on ...   \n",
       "97    instead the detainees remain stuck in legal li...   \n",
       "713   had down syndrome or was crippled mentally cha...   \n",
       "754   only about 48 percent of current working house...   \n",
       "...                                                 ...   \n",
       "1184  his journey has included a stint playing on hi...   \n",
       "212   this must be the place sean penn stars in this...   \n",
       "978   hence we need not be overly concerned about ci...   \n",
       "278   roger said that there were people who came on ...   \n",
       "473   that's because her work involve placing senior...   \n",
       "\n",
       "                                                phrases  \\\n",
       "554   ['multiple_partners', 'and', 'same_sex', 'part...   \n",
       "67    ['this', 'was', 'such', 'a', 'tragic_case', 'a...   \n",
       "97    ['instead', 'the', 'detainees', 'remain_stuck'...   \n",
       "713   ['had', 'down_syndrome', 'or', 'was', 'cripple...   \n",
       "754   ['only', 'about', '48_percent', 'of', 'current...   \n",
       "...                                                 ...   \n",
       "1184  ['his_journey', 'has', 'included_a_stint', 'pl...   \n",
       "212   ['this', 'must_be', 'the', 'place', 'sean_penn...   \n",
       "978   ['hence', 'we_need', 'not', 'be', 'overly_conc...   \n",
       "278   ['roger_said', 'that', 'there_were', 'people_w...   \n",
       "473   [\"that's\", 'because', 'her', 'work', 'involve_...   \n",
       "\n",
       "                                        quality_phrases  keyword_present  \n",
       "554   ['multiple_partners', 'same_sex', 'partners', ...                1  \n",
       "67    ['tragic_case', 'suspicion_of_felony', 'dui', ...                1  \n",
       "97         ['detainees', 'remain_stuck', 'legal_limbo']                1  \n",
       "713   ['down_syndrome', 'crippled', 'mentally_challe...                1  \n",
       "754   ['48_percent', 'working_households', 'ready', ...                1  \n",
       "...                                                 ...              ...  \n",
       "1184  ['his_journey', 'included_a_stint', 'playing',...                1  \n",
       "212   ['sean_penn', 'comedy', 'retired', 'rock_star'...                1  \n",
       "978      ['overly_concerned', 'collateral_damage', \"'\"]                1  \n",
       "278                 ['people_who', 'touch', 'rear_end']                1  \n",
       "473   ['involve_placing', 'senior_citizens', 'white_...                1  \n",
       "\n",
       "[1382 rows x 10 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load checkpoint containing quality phrases\n",
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_with_Quality_Phrases_1.csv', encoding='utf-8', index_col = 0)\n",
    "euph_corpus = euph_corpus.drop(euph_corpus[euph_corpus.is_euph == 0].index) # drop all non-euph rows\n",
    "euph_corpus = euph_corpus.sample(frac=1) # randomize row order\n",
    "\n",
    "import re\n",
    "euph_corpus['keyword_present'] = 0\n",
    "# denote rows where keyword was present in REGULAR phrases\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    if (euph_corpus.loc[i, \"is_euph\"] == 0):\n",
    "        continue\n",
    "    quality_phrases = euph_corpus.loc[i, \"quality_phrases\"]\n",
    "    # Converting string to list\n",
    "    quality_phrases = ast.literal_eval(quality_phrases)\n",
    "    keyword = euph_corpus.loc[i, 'keyword']\n",
    "    for q in quality_phrases:\n",
    "        q_string = re.sub(r'_', ' ', q)\n",
    "        if keyword in q_string:\n",
    "            euph_corpus.loc[i, 'keyword_present'] = 1\n",
    "            # count += 1\n",
    "            break\n",
    "\n",
    "euph_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8cd675-e54c-4b51-a603-6ac13772d4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2350434, 3241180)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load w2v model if using checkpoint\n",
    "data = [] # holds phrased input sentences to update wv model with\n",
    "\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    data.append(euph_corpus.loc[i, 'phrases'])\n",
    "    \n",
    "# define model and train on new data\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"data/wv_model_7\") # typically takes 45-90 seconds\n",
    "# train model on input data \n",
    "model.train(data, total_examples=len(data), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ca372-a68c-40a0-b74c-de85495a7dda",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### roBERTa Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e50b4183-9386-4227-9a77-39af5819c9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "def load_roberta_sentiment():\n",
    "    # Tasks:\n",
    "    # emoji, emotion, hate, irony, offensive, sentiment\n",
    "    # stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "    task='sentiment'\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "    # download label mapping\n",
    "    labels=[]\n",
    "    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "    with urllib.request.urlopen(mapping_link) as f:\n",
    "        html = f.read().decode('utf-8').split(\"\\n\")\n",
    "        csvreader = csv.reader(html, delimiter='\\t')\n",
    "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "    # pretrained\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    model.save_pretrained(MODEL)\n",
    "    tokenizer.save_pretrained(MODEL)\n",
    "    \n",
    "    return labels, model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2d014-f2c2-45d1-86aa-638521641334",
   "metadata": {},
   "source": [
    "#### roBERTa Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ec1e38-0915-4a24-8b40-3d4ab7e13a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_roberta_offensive():\n",
    "    task='offensive'\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "    # download label mapping\n",
    "    labels=[]\n",
    "    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "    with urllib.request.urlopen(mapping_link) as f:\n",
    "        html = f.read().decode('utf-8').split(\"\\n\")\n",
    "        csvreader = csv.reader(html, delimiter='\\t')\n",
    "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "    # PT\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    model.save_pretrained(MODEL)\n",
    "    tokenizer.save_pretrained(MODEL)\n",
    "    \n",
    "    return labels, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4893e394-9a6b-49ae-8d40-8cf8a6253ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for using the roberta models\n",
    "def get_sentiment(s, labels, model, tokenizer):\n",
    "    encoded_input = tokenizer(s, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    # ranking = np.argsort(scores)\n",
    "    # ranking = ranking[::-1]\n",
    "    # for i in range(scores.shape[0]):\n",
    "    #     l = labels[ranking[i]]\n",
    "    #     s = scores[ranking[i]]\n",
    "        # print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "    return scores\n",
    "\n",
    "def get_offensive(s, labels, model, tokenizer):\n",
    "    encoded_input = tokenizer(s, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    # ranking = np.argsort(scores)\n",
    "    # ranking = ranking[::-1]\n",
    "    # for i in range(0, 2):\n",
    "    #     l = labels[ranking[i]]\n",
    "    #     s = scores[ranking[i]]\n",
    "        # print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5eaa9-5ad6-4619-984a-19fedb07e4ce",
   "metadata": {},
   "source": [
    "#### Run sentiment/offensive analysis on euph corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "659d594a-1069-4d20-8106-d6f6f592ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "needs functions load_roberta_sentiment(), load_roberta_offensive(), get_sentiment() and get_offensive()\n",
    "'''\n",
    "\n",
    "# TEMP EXPERIMENT CHUNK\n",
    "from difflib import SequenceMatcher\n",
    "def get_similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def get_top_euph_candidates(text, phrases, num_paraphrases, wv_model, sentiment_pack, offensive_pack, show_stats=False):\n",
    "    \n",
    "    sentiment_labels, sentiment_model, sentiment_tokenizer = sentiment_pack[0], sentiment_pack[1], sentiment_pack[2]\n",
    "    offensive_labels, offensive_model, offensive_tokenizer = offensive_pack[0], offensive_pack[1], offensive_pack[2]\n",
    "    \n",
    "    orig_scores = list(get_sentiment(text, sentiment_labels, sentiment_model, sentiment_tokenizer))\n",
    "    orig_scores = orig_scores + list(get_offensive(text, offensive_labels, offensive_model, offensive_tokenizer))\n",
    "    if show_stats == True: print('SENTIMENT OF ORIGINAL SENTENCE: {}'.format(orig_scores))\n",
    "    phrase_scores = []\n",
    "\n",
    "    for q in phrases:\n",
    "        paraphrases = []\n",
    "        if show_stats == True: print('\\n'+q)\n",
    "        paraphrases = wv_model.wv.most_similar(q, topn = num_paraphrases) # can swap out\n",
    "        \n",
    "        # print(paraphrases)\n",
    "        \n",
    "        # various sentiment statistics\n",
    "        sentiment_shift = [0, 0, 0, 0, 0]\n",
    "        max_inc = [0, 0, 0, 0, 0]\n",
    "        max_inc_para = [\"\", \"\", \"\", \"\", \"\"]\n",
    "        tot_neg_inc = 0\n",
    "        tot_neu_inc = 0\n",
    "        tot_pos_inc = 0\n",
    "        tot_off_inc = 0\n",
    "        tot_noff_inc = 0\n",
    "        \n",
    "        # for length ratio feature\n",
    "        length_ratio = 0\n",
    "        tot_para_length = 0\n",
    "        num_para = 0\n",
    "        \n",
    "        for p in paraphrases:\n",
    "            p_string = re.sub(r'_', ' ', p[0]) # the underscores are removed for sentiment computation - experiment?\n",
    "            q_string = re.sub(r'_', ' ', q)\n",
    "            \n",
    "            '''FILTERING PARAPHRASES'''\n",
    "            # filtering out paraphrases if they're a superstring\n",
    "            if (q_string in p_string):\n",
    "#                 print(\"Paraphrase is superstring, skipping!\")\n",
    "#                 print()\n",
    "                continue\n",
    "    \n",
    "            # filtering out paraphrases if they're too similar\n",
    "            if (get_similarity(q_string, p_string) > 0.5):\n",
    "                continue\n",
    "                \n",
    "            if ('fuck' in p_string):\n",
    "                continue\n",
    "            \n",
    "            if (wv_model.wv.get_vecattr(p[0], 'count') < 5):\n",
    "                continue\n",
    "                \n",
    "            '''END PARAPHRASE FILTERING'''\n",
    "            \n",
    "            # replacement\n",
    "            pattern = re.compile(r'\\b'+q_string+r'\\b', re.IGNORECASE)\n",
    "            new_sentence = pattern.sub(p_string, text)\n",
    "            # at this point, we could check the integrity of the paraphrase\n",
    "\n",
    "            # get the sentiment/offensive scores for this paraphrase\n",
    "            scores = list(get_sentiment(new_sentence, sentiment_labels, sentiment_model, sentiment_tokenizer))\n",
    "            scores = scores + list(get_offensive(new_sentence, offensive_labels, offensive_model, offensive_tokenizer))\n",
    "\n",
    "            # update the quality phrase's sentiment statistics with the sentiment shifts from this paraphrase\n",
    "            shifts = [0, 0, 0, 0, 0]\n",
    "            for i in range(0, len(scores)):\n",
    "                shifts[i] = scores[i] - orig_scores[i]\n",
    "                sentiment_shift[i] += shifts[i]\n",
    "                if (shifts[i] > max_inc[i]):\n",
    "                    max_inc[i] = shifts[i]\n",
    "                    max_inc_para[i] = p_string\n",
    "\n",
    "            # update the relevant scores for detection\n",
    "            if (shifts[0] > 0):\n",
    "                tot_neg_inc += shifts[0]\n",
    "            if (shifts[1] > 0):\n",
    "                tot_neu_inc += shifts[1]\n",
    "            if (shifts[2] > 0):\n",
    "                tot_pos_inc += shifts[2]\n",
    "            if (shifts[3] > 0):\n",
    "                tot_noff_inc += shifts[3]\n",
    "            if (shifts[4] > 0):\n",
    "                tot_off_inc += shifts[4]\n",
    "                \n",
    "            # update counts for length ratio\n",
    "            num_para += 1\n",
    "            tot_para_length += len(p_string)\n",
    "        \n",
    "        # compute length ratio feature\n",
    "        if (num_para != 0):\n",
    "            avg_para_length = tot_para_length / num_para\n",
    "            length_ratio = len(q_string) / avg_para_length\n",
    "#         print(length_ratio)\n",
    "#         break\n",
    "        \n",
    "        for val in sentiment_shift:\n",
    "            val /= num_paraphrases\n",
    "        if (show_stats == True):\n",
    "            print(\"AVERAGE SENTIMENT SHIFTS: {}\".format(sentiment_shift))\n",
    "            print(\"MAX INCREASE FROM A PHRASE: {}\".format(max_inc))\n",
    "            print(\"PHRASES THAT CAUSED EACH ^: {}\".format(max_inc_para))\n",
    "            print(\"TOTAL NEGATIVE INCREASE: {}\".format(tot_neg_inc))\n",
    "            print(\"TOTAL NEUTRAL INCREASE: {}\".format(tot_neu_inc))\n",
    "            print(\"TOTAL NEUTRAL INCREASE: {}\".format(tot_noff_inc))\n",
    "            print(\"TOTAL OFFENSIVE INCREASE: {}\".format(tot_off_inc))\n",
    "\n",
    "        # phrase_scores.append((q_string, tot_neg_inc + tot_neu_inc + 2*(tot_noff_inc + tot_off_inc)))\n",
    "        # phrase_scores.append((q_string, 0.02337676*tot_neg_inc + 0.02267515*tot_neu_inc + 0.09802046*tot_noff_inc + 0.14442855*tot_off_inc))\n",
    "        # 0.01125929*tot_pos_inc\n",
    "        phrase_scores.append((q_string, 0.02183689*tot_neg_inc + 0.01949368*tot_neu_inc + 0.09130243*tot_noff_inc + 0.12983809 *tot_off_inc + 0.15030182*length_ratio))\n",
    "            \n",
    "        # phrase_scores.append((q_string, max_inc[0] + 2*max_inc[4]))\n",
    "        # phrase_scores.append((q_string, tot_neg_inc + tot_neu_inc + 2*(tot_off_inc)))\n",
    "    phrase_scores = list(sorted(phrase_scores, key=lambda x: x[1], reverse=True))\n",
    "    return phrase_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45b60dec-e610-437c-833d-8113f0df9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models\n",
    "sentiment_labels, sentiment_model, sentiment_tokenizer = load_roberta_sentiment()\n",
    "offensive_labels, offensive_model, offensive_tokenizer = load_roberta_offensive()\n",
    "\n",
    "sentiment_pack = [sentiment_labels, sentiment_model, sentiment_tokenizer]\n",
    "offensive_pack = [offensive_labels, offensive_model, offensive_tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fe47ae8-342f-4071-a9d0-d490a6afbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "num_paraphrases = 25\n",
    "score = 0\n",
    "k = 2 # check the top k candidates for the PET -> success\n",
    "euph_corpus['candidates'] = \"\"\n",
    "euph_corpus['top_2'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e8202d5-38d8-48fa-941d-92906b67e0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 87/1382 [10:23<1:39:43,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 182/1382 [23:54<2:28:50,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 269/1382 [35:59<2:16:42,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 358/1382 [46:29<2:10:46,  7.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 441/1382 [57:23<1:20:02,  5.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 539/1382 [1:10:37<1:07:56,  4.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 630/1382 [1:24:06<1:40:42,  8.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 716/1382 [1:34:52<1:26:01,  7.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 806/1382 [1:46:34<59:16,  6.18s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 888/1382 [1:57:49<45:01,  5.47s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 979/1382 [2:07:45<51:46,  7.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 1064/1382 [2:18:03<30:30,  5.76s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1150/1382 [2:29:53<21:39,  5.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 1225/1382 [2:38:11<17:58,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 1305/1382 [2:47:04<06:51,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1382/1382 [2:58:11<00:00,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euphemism detected in 798 out of 1382 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import ast\n",
    "\n",
    "for i, row in tqdm(euph_corpus.iterrows(), total=euph_corpus.shape[0]):\n",
    "# uncomment below if resuming from checkpoint\n",
    "#     if (0 < i < 600):\n",
    "#         continue\n",
    "    phrases = euph_corpus.loc[i, 'quality_phrases']\n",
    "    \n",
    "    # Converting string to list IF READING FROM CSV as checkpoint\n",
    "    phrases = ast.literal_eval(phrases)\n",
    "    \n",
    "    text = euph_corpus.loc[i, 'sentence']\n",
    "    euph = euph_corpus.loc[i, 'keyword']\n",
    "    \n",
    "    top_candidates = get_top_euph_candidates(text, phrases, num_paraphrases, model, \n",
    "                                             sentiment_pack, offensive_pack, show_stats=False)\n",
    "    \n",
    "#     print(top_candidates)\n",
    "#     print()\n",
    "    euph_corpus.at[i, 'candidates'] = top_candidates\n",
    "    \n",
    "    # check the top k candidates - this code could use cleaning up\n",
    "    for x in range(0, k):\n",
    "        if (len(top_candidates) == 0):\n",
    "            break\n",
    "        if (len(top_candidates) == 1):\n",
    "            candidate = top_candidates[0][0]\n",
    "            if euph in candidate:\n",
    "                score += 1\n",
    "                if (score % 50 == 0):\n",
    "                    print(score)\n",
    "                euph_corpus.loc[i, 'top_2'] = 1\n",
    "            break\n",
    "        candidate = top_candidates[x][0]\n",
    "        if euph in candidate:\n",
    "            score += 1\n",
    "            if (score % 50 == 0):\n",
    "                print(score)\n",
    "            euph_corpus.loc[i, 'top_2'] = 1\n",
    "            break\n",
    "\n",
    "    if (i == 691):\n",
    "        euph_corpus.to_csv('CHECKPOINT.csv')\n",
    "    if (i == 1382):\n",
    "        break\n",
    "print(\"Euphemism detected in {} out of {} sentences\".format(score, 1382))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b2a33a3-614f-41bf-971a-44b935dd04a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "725\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "for x in euph_corpus['top_2'].tolist():\n",
    "    if (x == 1):\n",
    "        num_correct += 1\n",
    "print(num_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f985515b-beed-4369-a6b6-7f3f3a5e984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "euph_corpus.to_csv('results_10.3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9fce9-5a69-4a5a-aa2a-d600e7718903",
   "metadata": {},
   "source": [
    "## Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38148a65-22d2-482d-8514-e31af5e67518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "euph_corpus = pd.read_csv('results_8.3.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30e720-0af6-4f24-b19e-9edef94a2347",
   "metadata": {},
   "source": [
    "#### Print number of 1st, 2nd, and 3rd place PET rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2188bcb-fe52-459f-b3c0-154277887c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "555\n",
      "243\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "import ast # this package is helpful for parsing lists stored in CSV files; which contain the literal characters [, ], etc.\n",
    "num_first_place = 0\n",
    "num_second_place = 0\n",
    "num_third_place = 0\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    if (i > 1382):\n",
    "        continue\n",
    "    top_2 = euph_corpus.loc[i, 'top_2']\n",
    "    keyword = euph_corpus.loc[i, 'keyword']\n",
    "    candidates = euph_corpus.loc[i, 'candidates']\n",
    "    # Converting string to list\n",
    "    # candidates = ast.literal_eval(candidates)\n",
    "    if (top_2 == 1):\n",
    "        if (keyword in candidates[0][0]):\n",
    "            num_first_place += 1\n",
    "        elif (keyword in candidates[1][0]):\n",
    "            num_second_place += 1\n",
    "    elif (len(candidates) > 2):\n",
    "        if (keyword in candidates[2][0]):\n",
    "            num_third_place += 1\n",
    "\n",
    "print(num_first_place)\n",
    "print(num_second_place)\n",
    "print(num_third_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67639fe6-6501-48b5-8d9f-2eeb9068ecb1",
   "metadata": {},
   "source": [
    "#### Print number of phrase candidates and target PETs retained after Phrase Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dabeae64-67c2-4436-8f50-fd3d15c96213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1251\n",
      "31348\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "count = 0\n",
    "tot_p = 0\n",
    "# denote rows where keyword was present in REGULAR phrases\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    if (euph_corpus.loc[i, \"is_euph\"] == 0):\n",
    "        continue\n",
    "    phrases = euph_corpus.loc[i, \"phrases\"]\n",
    "    # Converting string to list\n",
    "    phrases = ast.literal_eval(phrases)\n",
    "    tot_p += len(phrases)\n",
    "    keyword = euph_corpus.loc[i, 'keyword']\n",
    "    for p in phrases:\n",
    "        p_string = re.sub(r'_', ' ', p)\n",
    "        if keyword in p_string:\n",
    "            euph_corpus.loc[i, 'keyword_present'] = 1\n",
    "            count += 1\n",
    "            break\n",
    "            \n",
    "print(count)\n",
    "print(tot_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f563aeb1-7f30-4613-adab-c3374189a5d6",
   "metadata": {},
   "source": [
    "#### Print number of phrase candidates and target PETs retained after Phrase Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da585b8f-319f-4830-bbff-e131a8376572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199\n",
      "10492\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "count = 0\n",
    "tot_q = 0\n",
    "# denote rows where keyword was present in quality phrases\n",
    "euph_corpus['keyword_present'] = 0\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    if (euph_corpus.loc[i, \"is_euph\"] == 0):\n",
    "        continue\n",
    "    quality_phrases = euph_corpus.loc[i, \"quality_phrases\"]\n",
    "    # Converting string to list\n",
    "    quality_phrases = ast.literal_eval(quality_phrases)\n",
    "    tot_q += len(quality_phrases)\n",
    "    keyword = euph_corpus.loc[i, 'keyword']\n",
    "    for q in quality_phrases:\n",
    "        q_string = re.sub(r'_', ' ', q)\n",
    "        if keyword in q_string:\n",
    "            euph_corpus.loc[i, 'keyword_present'] = 1\n",
    "            count += 1\n",
    "            break\n",
    "\n",
    "print(count)\n",
    "print(tot_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bed96f10-9af5-4b53-9535-67b56a74c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# append whether or not the PET is present in quality_phrases (prior to ranking stage) as a column\n",
    "euph_corpus.to_csv('results_8.2.1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15101dce-5a50-4a5a-a0e4-00537007d6b2",
   "metadata": {},
   "source": [
    "#### Print number of phrase candidates and target PETs retained after Phrase Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d55b51f-ef74-4f96-8d0a-1b10076e2acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2727\n"
     ]
    }
   ],
   "source": [
    "tot_top_2 = 0\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    if (euph_corpus.loc[i, \"is_euph\"] == 0):\n",
    "        continue\n",
    "    phrases = euph_corpus.loc[i, \"candidates\"]\n",
    "    if (len(phrases) == 0):\n",
    "        continue\n",
    "    if (len(phrases) == 1):\n",
    "        tot_top_2 += 1\n",
    "    else: \n",
    "        tot_top_2 += 2\n",
    "    # Converting string to list\n",
    "    # phrases = ast.literal_eval(phrases)\n",
    "    # tot_top_2 += len(phrases)\n",
    "print(tot_top_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b641e6a3-8bc0-4d0d-b269-c1858aa8ec19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
