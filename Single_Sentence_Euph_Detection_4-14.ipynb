{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183860a6-45a0-4d3d-91a1-7b5ef05e1425",
   "metadata": {},
   "source": [
    "# Test a sentence input\n",
    "Requires these files in the same folder as this notebook:\n",
    "- a bigram_phraser\n",
    "- a trigram_phraser\n",
    "- a word2vec model (3 files: model, syn1neg, and vectors)\n",
    "- a list of stopwords (to ignore as potential euphemisms)\n",
    "\n",
    "Required packages:\n",
    "- gensim\n",
    "- transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e594810c-bbc6-4577-a917-b5d1c3a09ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2a9cb65-a08b-4e80-8ceb-e512be404724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Euph_Detection:\n",
    "    def __init__(self, bigram_phraser, trigram_phraser, w2v_model, stopwords_text, sentiment, offensive):\n",
    "        self.bigram_phraser = Phraser.load(bigram_phraser)\n",
    "        self.trigram_phraser = Phraser.load(trigram_phraser)\n",
    "        self.model = Word2Vec.load(w2v_model)\n",
    "        # self.model = KeyedVectors.load(w2v_model)\n",
    "        self.topic_list = ['politics', 'death', 'kill', 'crime',\n",
    "               'drugs', 'alcohol', 'fat', 'old', 'poor', 'cheap',\n",
    "               'sex', 'sexual',\n",
    "               'employment', 'job', 'disability', 'disabled', \n",
    "               'accident', 'pregnant', 'poop', 'sickness', 'race', 'racial', 'vomit'\n",
    "              ]\n",
    "        self.stopwords = self.read_stopwords(stopwords_text)\n",
    "        # load the sentiment models\n",
    "        # sentiment_labels, sentiment_model, sentiment_tokenizer = self.load_roberta(sentiment)\n",
    "        # offensive_labels, offensive_model, offensive_tokenizer = self.load_roberta(offensive)\n",
    "        # pack them together - just for conciseness\n",
    "        self.sentiment_pack = [x for x in self.load_roberta(sentiment)]\n",
    "        self.offensive_pack = [x for x in self.load_roberta(offensive)]\n",
    "    \n",
    "    def preprocess(self, s):\n",
    "        s = s.strip()\n",
    "        s = re.sub(r'(##\\d*\\W)|<\\w>|,|;|:|--|\\(|\\)|#|%|\\\\|\\/|\\.|\\*|\\+|@', '', s)\n",
    "        s = re.sub(r'\\s\\s+', ' ', s)\n",
    "        s = s.lower()\n",
    "        return s\n",
    "\n",
    "    def get_phrases(self, s):\n",
    "        bigrammed_phrases = self.bigram_phraser[s.split()]\n",
    "        trigrammed_phrases = self.trigram_phraser[bigrammed_phrases]\n",
    "\n",
    "    def sum_similarity(self, phrase, topic_list):\n",
    "        score = 0\n",
    "        for topic in topic_list:\n",
    "            try:\n",
    "                similarity = self.model.wv.similarity(phrase, topic)\n",
    "                # EXPERIMENTAL - to \"reward\" the phrases with a high similarity to a particular category, but maybe not others\n",
    "                if (similarity > 0.50):\n",
    "                    # print(\"{} has a high similarity with {}\".format(phrase, topic))\n",
    "                    return 1.51\n",
    "                if (similarity > 0):\n",
    "                    score += similarity\n",
    "            except:\n",
    "                score += 0\n",
    "        return score\n",
    "    \n",
    "    def read_stopwords(self, text):\n",
    "        stopwords = []\n",
    "        with open(text,'rb') as f:\n",
    "            content = f.read()\n",
    "            content = content.split(b'\\r\\n')\n",
    "            for line in content:\n",
    "                stopwords.append(line.decode('utf-8'))\n",
    "        return stopwords\n",
    "\n",
    "    def topically_filter_phrases(self, phrases, topic_list, stopwords, THRESHOLD, show_stats=False):\n",
    "        quality_phrases = []\n",
    "        filtered = []\n",
    "\n",
    "        for phrase in phrases:\n",
    "            if (phrase in stopwords):\n",
    "                continue\n",
    "            similarity = self.sum_similarity(phrase, topic_list)\n",
    "\n",
    "            if (show_stats == True):\n",
    "                print(\"{} has a relevance score of {}\".format(phrase, similarity)) #table?\n",
    "\n",
    "            if (similarity > THRESHOLD and phrase not in quality_phrases):\n",
    "                quality_phrases.append(phrase)\n",
    "            else:\n",
    "                filtered.append(phrase)\n",
    "\n",
    "        # if (show_stats == True):\n",
    "        #     print(\"\\nRELEVANT PHRASES: {}\".format(quality_phrases))\n",
    "        #     print(\"IRRELEVANT PHRASES: {}\".format(filtered))\n",
    "        return quality_phrases\n",
    "\n",
    "    def load_roberta(self, task):\n",
    "        # Tasks:\n",
    "        # emoji, emotion, hate, irony, offensive, sentiment\n",
    "        # stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "        # task='sentiment' or 'offensive'\n",
    "        MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "        # download label mapping\n",
    "        labels=[]\n",
    "        mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "        with urllib.request.urlopen(mapping_link) as f:\n",
    "            html = f.read().decode('utf-8').split(\"\\n\")\n",
    "            csvreader = csv.reader(html, delimiter='\\t')\n",
    "        labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "        # pretrained\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "        model.save_pretrained(MODEL)\n",
    "        tokenizer.save_pretrained(MODEL)\n",
    "\n",
    "        return labels, model, tokenizer\n",
    "        \n",
    "    '''\n",
    "    functions for getting the sentiment \n",
    "    '''\n",
    "    def get_sentiment(self, s, pack):\n",
    "        labels, model, tokenizer = pack[0], pack[1], pack[2]\n",
    "        encoded_input = tokenizer(s, return_tensors='pt')\n",
    "        output = model(**encoded_input)\n",
    "        scores = output[0][0].detach().numpy()\n",
    "        scores = softmax(scores)\n",
    "        return scores\n",
    "    \n",
    "    '''\n",
    "    needs functions load_roberta_sentiment(), load_roberta_offensive(), get_sentiment() and get_offensive()\n",
    "    '''\n",
    "    def get_top_euph_candidates(self, text, phrases, num_paraphrases, wv_model, sentiment_pack, offensive_pack, show_stats=False):\n",
    "        orig_scores = list(self.get_sentiment(text, sentiment_pack))\n",
    "        orig_scores = orig_scores + list(self.get_sentiment(text, offensive_pack))\n",
    "        if show_stats == True: print('SENTIMENT OF ORIGINAL SENTENCE: {}'.format(orig_scores))\n",
    "        phrase_scores = []\n",
    "\n",
    "        for q in tqdm(phrases):\n",
    "            paraphrases = []\n",
    "            if show_stats == True: print('\\n'+q)\n",
    "            paraphrases = wv_model.wv.most_similar(q, topn = num_paraphrases) # can swap out\n",
    "            # print(q)\n",
    "            # print(paraphrases)\n",
    "            \n",
    "            # various sentiment statistics\n",
    "            sentiment_shift = [0, 0, 0, 0, 0] # [neg, neu, pos, off, n-off]\n",
    "            max_inc = [0, 0, 0, 0, 0]\n",
    "            max_inc_para = [\"\", \"\", \"\", \"\", \"\"]\n",
    "            tot_neg_inc = 0\n",
    "            tot_neu_inc = 0\n",
    "            tot_pos_inc = 0\n",
    "            tot_noff_inc = 0\n",
    "            tot_off_inc = 0\n",
    "            \n",
    "            for p in paraphrases:\n",
    "                p_string = re.sub(r'_', ' ', p[0]) # the underscores are removed for sentiment computation - experiment?\n",
    "                q_string = re.sub(r'_', ' ', q)\n",
    "\n",
    "                # replacement\n",
    "                pattern = re.compile(r'\\b'+q_string+r'\\b', re.IGNORECASE)\n",
    "                new_sentence = pattern.sub(p_string, text)\n",
    "                # at this point, we could check the integrity of the paraphrase\n",
    "\n",
    "                # get the sentiment/offensive scores for this paraphrase\n",
    "                scores = list(self.get_sentiment(new_sentence, sentiment_pack))\n",
    "                scores = scores + list(self.get_sentiment(new_sentence, offensive_pack))\n",
    "\n",
    "                # update the quality phrase's sentiment statistics with the sentiment shifts from this paraphrase\n",
    "                shifts = [0, 0, 0, 0, 0]\n",
    "                for i in range(0, len(scores)):\n",
    "                    shifts[i] = scores[i] - orig_scores[i]\n",
    "                    sentiment_shift[i] += shifts[i]\n",
    "                    if (shifts[i] > max_inc[i]):\n",
    "                        max_inc[i] = shifts[i]\n",
    "                        max_inc_para[i] = p_string\n",
    "\n",
    "                # update the relevant scores for detection\n",
    "                if (shifts[0] > 0):\n",
    "                    tot_neg_inc += shifts[0]\n",
    "                if (shifts[1] > 0):\n",
    "                    tot_neu_inc += shifts[1]\n",
    "                if (shifts[2] > 0):\n",
    "                    tot_pos_inc += shifts[2]\n",
    "                if (shifts[3] > 0):\n",
    "                    tot_noff_inc += shifts[3]\n",
    "                if (shifts[4] > 0):\n",
    "                    tot_off_inc += shifts[4]\n",
    "                \n",
    "                # print(p_string)\n",
    "                # print(shifts)\n",
    "\n",
    "            for val in sentiment_shift:\n",
    "                val /= num_paraphrases\n",
    "            if (show_stats == True):\n",
    "                print(\"AVERAGE SENTIMENT SHIFTS: {}\".format(sentiment_shift))\n",
    "                print(\"MAX INCREASE FROM A PHRASE: {}\".format(max_inc))\n",
    "                print(\"PHRASES THAT CAUSED EACH ^: {}\".format(max_inc_para))\n",
    "                print(\"TOTAL NEGATIVE INCREASE: {}\".format(tot_neg_inc))\n",
    "                print(\"TOTAL NEUTRAL INCREASE: {}\".format(tot_neu_inc))\n",
    "                print(\"TOTAL POSITIVE INCREASE: {}\".format(tot_pos_inc))\n",
    "                print(\"TOTAL NON-OFFENSIVE INCREASE: {}\".format(tot_noff_inc))\n",
    "                print(\"TOTAL OFFENSIVE INCREASE: {}\".format(tot_off_inc))\n",
    "\n",
    "            # score = tot_neg_inc + tot_neu_inc + + tot_pos_inc + tot_noff_inc + tot_off_inc\n",
    "            score = tot_neg_inc + tot_neu_inc + tot_pos_inc + 2*(tot_noff_inc + tot_off_inc)\n",
    "            # score = sentiment_shift[0]+sentiment_shift[1]+sentiment_shift[3]\n",
    "            # score = (tot_off_inc/tot_neg_inc)*(tot_neg_inc + tot_neu_inc + tot_off_inc)\n",
    "            phrase_scores.append((q_string, score))\n",
    "\n",
    "        phrase_scores = list(sorted(phrase_scores, key=lambda x: x[1], reverse=True))\n",
    "        return phrase_scores\n",
    "    \n",
    "    def detect_euphs(self, s, topic_threshold, num_paraphrases, show_stats=False):\n",
    "        s = self.preprocess(s)\n",
    "        print(s)\n",
    "\n",
    "        bigrammed_phrases = self.bigram_phraser[s.split()]\n",
    "        trigrammed_phrases = self.trigram_phraser[bigrammed_phrases]\n",
    "        print(\"\\nDETECTED PHRASES: {}\".format(trigrammed_phrases))\n",
    "\n",
    "        data = []\n",
    "        data.append(trigrammed_phrases)\n",
    "        # train model on input data\n",
    "        self.model.train(data, total_examples=len(data), epochs=10)\n",
    "\n",
    "        quality_phrases = self.topically_filter_phrases(trigrammed_phrases, self.topic_list, self.stopwords, topic_threshold, show_stats)\n",
    "        print(\"\\nRELEVANT PHRASES: {}\".format(quality_phrases))\n",
    "\n",
    "        candidate_list = self.get_top_euph_candidates(s, quality_phrases, num_paraphrases, \n",
    "                                            self.model, self.sentiment_pack, self.offensive_pack, \n",
    "                                            show_stats)\n",
    "        return candidate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6dd55a-0ba9-450b-8b83-994b81156ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "euph_detector = Euph_Detection('bigram_phraser_7', 'trigram_phraser_7', 'wv_model_7', 'stopwords.txt', 'sentiment', 'offensive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce303206-46ad-453d-8e81-8af62e74caa1",
   "metadata": {},
   "source": [
    "#### Input your sentence below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff95dfd0-9994-44ac-8d11-3f9bba425f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cant you tell she has a bun in the oven no not a hot dog bun arnold\n",
      "\n",
      "DETECTED PHRASES: ['cant', 'you', 'tell', 'she_has', 'a', 'bun_in_the_oven', 'no', 'not', 'a', 'hot_dog_bun', 'arnold']\n",
      "\n",
      "RELEVANT PHRASES: ['she_has', 'bun_in_the_oven', 'hot_dog_bun']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:06<00:00,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EUPH CANDIDATE RANKING: [('bun in the oven', 7.889018282294273), ('hot dog bun', 4.71666407212615), ('she has', 2.7513428330421448)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = 'cant you tell she has a bun in the oven, no not a hot dog bun arnold'\n",
    "\n",
    "candidate_ranking = euph_detector.detect_euphs(s, topic_threshold=1.45, num_paraphrases=25, show_stats=False)\n",
    "\n",
    "print(\"\\nEUPH CANDIDATE RANKING: {}\".format(candidate_ranking))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a7a913-ad6c-4dff-97ef-16d99de62021",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analyzing the Process\n",
    "After running Euph_Detection on a sentence, you can further look at the intermediate outputs for a specific candidate phrase:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a680ebea-a5bc-4c71-bc40-4d6a3a266048",
   "metadata": {},
   "source": [
    "#### Topic Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c992752-906b-4a98-9a6b-41b29f00b845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics: -0.10268321633338928\n",
      "death: 0.04841664433479309\n",
      "kill: 0.14225205779075623\n",
      "crime: -0.07305203378200531\n",
      "drugs: 0.007137034088373184\n",
      "alcohol: 0.002544146031141281\n",
      "fat: 0.21911388635635376\n",
      "old: 0.047357894480228424\n",
      "poor: 0.0035524219274520874\n",
      "cheap: 0.02102913334965706\n",
      "sex: 0.09789717197418213\n",
      "sexual: 0.04411625862121582\n",
      "employment: -0.06391395628452301\n",
      "job: 0.1524340808391571\n",
      "disability: 0.03895317390561104\n",
      "disabled: 0.06526339799165726\n",
      "accident: 0.01644478738307953\n",
      "pregnant: 0.2601613998413086\n",
      "poop: 0.24594008922576904\n",
      "sickness: 0.04668925330042839\n",
      "race: -0.03729817643761635\n",
      "racial: -0.06883380562067032\n",
      "vomit: 0.15760526061058044\n",
      "SIMILAR TOPICS: ['pregnant']\n",
      "TOTAL SCORE: 1.6169080920517445\n"
     ]
    }
   ],
   "source": [
    "test_phrase = 'bun_in_the_oven'\n",
    "similar_topics = []\n",
    "\n",
    "topic_list = euph_detector.topic_list\n",
    "model = euph_detector.model\n",
    "\n",
    "score = 0\n",
    "for topic in topic_list:\n",
    "    similarity = model.wv.similarity(test_phrase, topic)\n",
    "    if (similarity > 0.25):\n",
    "        similar_topics.append(topic)\n",
    "    if (similarity > 0):\n",
    "        score += similarity\n",
    "    print('{}: {}'.format(topic, similarity))\n",
    "\n",
    "print('SIMILAR TOPICS: {}'.format(similar_topics))\n",
    "print('TOTAL SCORE: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c727aec8-f2a7-484d-a9cc-c31ca8634b58",
   "metadata": {},
   "source": [
    "#### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "beaf9c28-1d80-4617-b957-0a7ebe01f0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTIMENT OF ORIGINAL SENTENCE: [0.15074506, 0.766159, 0.08309597, 0.66507876, 0.33492124]\n",
      "\n",
      "bun_in_the_oven\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kyle richards\n",
      "battle aura\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▋                                                                            | 2/25 [00:00<00:02, 11.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "been spayed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█████████████▎                                                                     | 4/25 [00:00<00:01, 11.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trachiniae\n",
      "choreographer/partner\n",
      "sash-bearing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|███████████████████▉                                                               | 6/25 [00:00<00:01, 11.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crown-wearing\n",
      "african violets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████████▌                                                        | 8/25 [00:00<00:01, 11.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2938490toolong\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████▊                                                 | 10/25 [00:00<00:01, 11.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mint basil\n",
      "narrows her eyes\n",
      "rynestead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|███████████████████████████████████████▎                                          | 12/25 [00:01<00:01, 11.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sunchoke\n",
      "dollop of whipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████████████████████████████████████████████▉                                    | 14/25 [00:01<00:00, 11.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bettie page\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|████████████████████████████████████████████████████▍                             | 16/25 [00:01<00:00, 11.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gadhia-smith\n",
      "cooked shrimp\n",
      "holosericea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████                       | 18/25 [00:01<00:00, 11.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pas trop\n",
      "wear a neck brace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████▌                | 20/25 [00:01<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "school/district\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████████████████████████████████████████████████████████████████████▏         | 22/25 [00:01<00:00, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tall athletic\n",
      "big chin\n",
      "manchego cheese\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 25/25 [00:02<00:00, 11.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eight-months-pregnant\n",
      "AVERAGE SENTIMENT SHIFTS: [0.31104613095521927, -0.4423208236694336, 0.13127442076802254, -0.8401100635528564, 0.8401102125644684]\n",
      "MAX INCREASE FROM A PHRASE: [0.13473663, 0.048584163, 0.07316646, 0.04799378, 0.125934]\n",
      "PHRASES THAT CAUSED EACH ^: ['been spayed', 'cooked shrimp', 'mint basil', 'battle aura', 'big chin']\n",
      "TOTAL NEGATIVE INCREASE: 0.6099239438772202\n",
      "TOTAL NEUTRAL INCREASE: 0.11755067110061646\n",
      "TOTAL POSITIVE INCREASE: 0.23823228478431702\n",
      "TOTAL NON-OFFENSIVE INCREASE: 0.14372360706329346\n",
      "TOTAL OFFENSIVE INCREASE: 0.9838335514068604\n",
      "('bun in the oven', 3.2208212167024612)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = s\n",
    "q = 'bun_in_the_oven'\n",
    "\n",
    "sentiment_pack = euph_detector.sentiment_pack\n",
    "offensive_pack = euph_detector.offensive_pack\n",
    "\n",
    "orig_scores = list(euph_detector.get_sentiment(s, sentiment_pack))\n",
    "orig_scores = orig_scores + list(euph_detector.get_sentiment(text, offensive_pack))\n",
    "print('SENTIMENT OF ORIGINAL SENTENCE: {}'.format(orig_scores))\n",
    "\n",
    "num_paraphrases=25\n",
    "paraphrases = []\n",
    "print('\\n'+q)\n",
    "paraphrases = model.wv.most_similar(q, topn = num_paraphrases) # can swap out\n",
    "\n",
    "# various sentiment statistics\n",
    "sentiment_shift = [0, 0, 0, 0, 0] # [neg, neu, pos, off, n-off]\n",
    "max_inc = [0, 0, 0, 0, 0]\n",
    "max_inc_para = [\"\", \"\", \"\", \"\", \"\"]\n",
    "tot_neg_inc = 0\n",
    "tot_neu_inc = 0\n",
    "tot_pos_inc = 0\n",
    "tot_noff_inc = 0\n",
    "tot_off_inc = 0\n",
    "\n",
    "for p in tqdm(paraphrases):\n",
    "    p_string = re.sub(r'_', ' ', p[0]) # the underscores are removed for sentiment computation - experiment?\n",
    "    q_string = re.sub(r'_', ' ', q)\n",
    "\n",
    "    # replacement\n",
    "    pattern = re.compile(r'\\b'+q_string+r'\\b', re.IGNORECASE)\n",
    "    new_sentence = pattern.sub(p_string, text)\n",
    "    print(p_string)\n",
    "    # at this point, we could check the integrity of the paraphrase\n",
    "\n",
    "    # get the sentiment/offensive scores for this paraphrase\n",
    "    scores = list(euph_detector.get_sentiment(new_sentence, sentiment_pack))\n",
    "    scores = scores + list(euph_detector.get_sentiment(new_sentence, offensive_pack))\n",
    "\n",
    "    # update the quality phrase's sentiment statistics with the sentiment shifts from this paraphrase\n",
    "    shifts = [0, 0, 0, 0, 0]\n",
    "    for i in range(0, len(scores)):\n",
    "        shifts[i] = scores[i] - orig_scores[i]\n",
    "        sentiment_shift[i] += shifts[i]\n",
    "        if (shifts[i] > max_inc[i]):\n",
    "            max_inc[i] = shifts[i]\n",
    "            max_inc_para[i] = p_string\n",
    "    # print(shifts)\n",
    "    # update the relevant scores for detection\n",
    "    if (shifts[0] > 0):\n",
    "        tot_neg_inc += shifts[0]\n",
    "    if (shifts[1] > 0):\n",
    "        tot_neu_inc += shifts[1]\n",
    "    if (shifts[2] > 0):\n",
    "        tot_pos_inc += shifts[2]\n",
    "    if (shifts[3] > 0):\n",
    "        tot_noff_inc += shifts[3]\n",
    "    if (shifts[4] > 0):\n",
    "        tot_off_inc += shifts[4]     \n",
    "\n",
    "    for val in sentiment_shift:\n",
    "        val /= num_paraphrases\n",
    "    \n",
    "print(\"AVERAGE SENTIMENT SHIFTS: {}\".format(sentiment_shift))\n",
    "print(\"MAX INCREASE FROM A PHRASE: {}\".format(max_inc))\n",
    "print(\"PHRASES THAT CAUSED EACH ^: {}\".format(max_inc_para))\n",
    "print(\"TOTAL NEGATIVE INCREASE: {}\".format(tot_neg_inc))\n",
    "print(\"TOTAL NEUTRAL INCREASE: {}\".format(tot_neu_inc))\n",
    "print(\"TOTAL POSITIVE INCREASE: {}\".format(tot_pos_inc))\n",
    "print(\"TOTAL NON-OFFENSIVE INCREASE: {}\".format(tot_noff_inc))\n",
    "print(\"TOTAL OFFENSIVE INCREASE: {}\".format(tot_off_inc))\n",
    "\n",
    "# score = 0.5*tot_neg_inc + 0.25*tot_neu_inc + 1.5*tot_off_inc\n",
    "score = tot_neg_inc + tot_neu_inc + tot_pos_inc + 2*(tot_noff_inc + tot_off_inc)\n",
    "print((q_string, score))\n",
    "# print((q_string, sentiment_shift[0]+sentiment_shift[1]+sentiment_shift[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2eb2ca-e6d7-47f2-af46-13ade4b1e4b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
