{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7db7b57-593f-402b-9e89-bd9b91b2ebfc",
   "metadata": {},
   "source": [
    "## Testing on euph corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a826d2-feed-40fa-82bc-b522d3378c67",
   "metadata": {},
   "source": [
    "#### Load and preprocess euph sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139ed7ad-2115-4b72-b97a-f1c37bd661c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "euph_corpus = pd.read_csv('Euphemism_Corpus_2-24.csv', index_col=0, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bf54fb6-1ff6-4b92-932b-a55b2f50d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(s):\n",
    "    s = s.strip()\n",
    "    s = re.sub(r'(##\\d*\\W)|<\\w>|,|;|:|--|\\(|\\)|#|%|\\\\|\\/|\\.|\\*|\\+|@', '', s)\n",
    "    s = re.sub(r'\\s\\s+', ' ', s)\n",
    "    s = s.lower()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "346edc29-5cd1-4af3-bfa0-22dc8c95dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess sentences\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    text = euph_corpus.loc[i, 'sentence']\n",
    "    euph_corpus.loc[i, 'sentence'] = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeef0f1e-1224-4a47-88ea-7524a33eb350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrasify the sentences\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "\n",
    "bigram_phraser = Phraser.load('bigram_phraser_7')\n",
    "trigram_phraser = Phraser.load('trigram_phraser_7')\n",
    "euph_corpus['phrases'] = \"\"\n",
    "data = [] # holds phrased input sentences to update wv model with\n",
    "\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    text = euph_corpus.loc[i, 'sentence']\n",
    "    euph_corpus.at[i, 'phrases'] = bigram_phraser[text.split()] # use phraser to detect phrases in text\n",
    "    euph_corpus.at[i, 'phrases'] = trigram_phraser[euph_corpus.loc[i, 'phrases']]\n",
    "    data.append(euph_corpus.loc[i, 'phrases'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26467c31-7785-47b9-9e21-694bcbaff4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'we',\n",
       " 'talking',\n",
       " 'the',\n",
       " 'merits',\n",
       " \"of'\",\n",
       " 'enhanced_interrogation_techniques',\n",
       " 'or',\n",
       " 'the',\n",
       " 'definition_of_torture']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm the phraser is still working\n",
    "trigram_phraser[['are', 'we', 'talking', 'the', 'merits', \"of'\", 'enhanced_interrogation', 'techniques', 'or', 'the', 'definition', 'of', 'torture']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e543dfa-0ea0-4c08-9c34-ef8e1ccdaa65",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Define topic similarity function, topic list and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35f09267-a883-4fc2-83b8-ef3ad151c679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_similarity(phrase, topic_list):\n",
    "    score = 0\n",
    "    for topic in topic_list:\n",
    "        try:\n",
    "            similarity = model.wv.similarity(phrase, topic)\n",
    "            if (similarity > 0):\n",
    "                score += similarity\n",
    "        except:\n",
    "            score += 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d6bd72d-a73a-4123-a655-e459d0e0c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define topic list and stopwords\n",
    "topic_list = ['politics', 'death', 'kill', 'crime',\n",
    "               'drugs', 'alcohol', 'fat', 'old', 'poor', 'cheap',\n",
    "               'sex', 'sexual',\n",
    "               'employment', 'job', 'disability', 'disabled', \n",
    "               'accident', 'pregnant', 'poop', 'sickness', 'race', 'racial', 'vomit'\n",
    "              ]\n",
    "\n",
    "stopwords = []\n",
    "#['the', 'a', 'to', 'him', 'her', 'them', 'me', 'you', 'of', 'with']\n",
    "\n",
    "with open('stopwords.txt','rb') as f:\n",
    "    content = f.read()\n",
    "    content = content.split(b'\\r\\n')\n",
    "    for line in content:\n",
    "        stopwords.append(line.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57967be1-b4ae-4b11-a476-4b337dd20d98",
   "metadata": {},
   "source": [
    "#### Perform topic filtering and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04c02c89-8dca-4152-b11d-449191d9807b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(346506, 451540)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model and train on new data\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load(\"wv_model_7\")\n",
    "# train model on input data \n",
    "model.train(data, total_examples=len(data), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "709d0eb6-daff-429c-9104-93859aa1ac70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retained the euphemism in 1627 out of 1965 sentences\n",
      "14776 quality phrases retained overall\n",
      "Filtered 7045 non-keywords out\n",
      "\n",
      "EXACT SUCCESSES: ['tinkle', 'undocumented immigrants', 'undocumented immigrant', 'venereal diseases', 'venereal disease', 'sex workers', 'sex worker', 'mentally disabled', 'correctional facilities', 'correctional facility', 'freedom fighters', 'freedom fighter', 'detainees', 'detainee', 'psychiatric hospital', 'ethnic cleansing', 'ethnically cleansed', 'enhanced interrogation techniques', 'mistruths', 'mistruth', 'elderly', 'armed conflict', 'drinking problem', 'deceased', 'pro-life', 'income inequality', 'rear end', 'lavatory', 'birds and the bees', 'inner city', 'developing country', 'developed country', 'substance abuse', 'global south', 'underprivileged', 'inebriated', 'homemaker', 'capital punishment', 'differently-abled', 'indigent', 'detention camp', 'pass gas', 'dearly departed', 'terminating a pregnancy', 'pregnancy termination', 'senior citizen', 'senior citizens', 'substance abuser', 'substance abusers', 'undocumented workers', 'pre-owned', 'sanitation workers', 'sanitation worker', 'latrine', 'plus-sized', 'physically challenged', 'developmentally disabled', 'custodians', 'same sex', 'able-bodied', 'hearing impaired', 'under the weather', 'people of color', 'adult beverages', 'fatalities', 'fatality', 'pro-choice', 'targeted killings', 'targeted killing', 'low-income', 'less fortunate', 'advanced age', 'mentally challenged', 'droppings', 'portly', 'negative cash flow', 'golden years', 'perished', 'perish', 'passed away', 'passing away', 'pass away', 'neutralize', 'neutralized', 'overweight', 'aging', 'chest', 'demise', 'slim', 'dismissed', 'sober', 'collateral damage', 'deprived', 'plump', 'same-sex', 'stout', 'disabled', 'special needs', 'disadvantaged', 'underdeveloped', 'invalid', 'mixed up', 'economical', 'over the hill', 'intoxicated', 'regime change', 'exterminate', 'custodian', 'put to sleep', 'downsize', 'laid off', 'laying off', 'lay off', 'went to heaven', 'accident', 'gluteus maximus', 'oldest profession', 'late', 'experienced', 'outspoken', 'wealthy', 'troubled', 'seeing someone', 'slept with', 'weed']\n",
      "\n",
      "PARTIAL SUCCESSES: ['undocumented immigrants', 'mentally disabled', 'detainees', 'ethnic cleansing', 'elderly', 'deceased', 'pro-life', 'income inequality', 'rear end', 'inner city', 'substance abuse', 'underprivileged', 'homemaker', 'indigent', 'pre-owned', 'same sex', 'able-bodied', 'fatality', 'fatalities', 'pro-choice', 'low-income', 'less fortunate', 'droppings', 'golden years', 'perished', 'perish', 'neutralize', 'overweight', 'aging', 'chest', 'demise', 'slim', 'dismissed', 'sober', 'collateral damage', 'plump', 'same-sex', 'stout', 'disabled', 'special needs', 'disadvantaged', 'underdeveloped', 'economical', 'intoxicated', 'regime change', 'exterminate', 'expecting', 'lay off', 'laid off', 'laying off', 'accident', 'late', 'outspoken', 'wealthy', 'troubled', 'weed', 'let them go', 'deprived', 'invalid', 'custodian', 'experienced', 'with child']\n",
      "\n",
      "FAILURES: ['undocumented immigrants', 'sex workers', 'mentally disabled', 'correctional facility', 'detainees', 'comfort women', 'ethnic cleansing', 'lost my lunch', 'lose your lunch', 'lose their lunch', 'enhanced interrogation techniques', 'elderly', 'deceased', 'pro-life', 'income inequality', 'rear end', 'inner city', 'developed country', 'developing country', 'substance abuse', 'underprivileged', 'homemaker', 'differently-abled', 'indigent', 'detention camp', 'economical with the truth', 'substance abusers', 'undocumented workers', 'street person', 'full figured', 'same sex', 'able-bodied', 'people of color', 'broken home', 'fatality', 'fatalities', 'pro-choice', 'low-income', 'less fortunate', 'advanced age', 'mentally challenged', 'droppings', 'time of the month', 'made love', 'making love', 'make love', 'running behind', 'let go of', 'let him go', 'letting someone go', 'perished', 'perish', 'passing on', 'pass on', 'neutralize', 'overweight', 'aging', 'chest', 'demise', 'slim', 'dismissed', 'sober', 'collateral damage', 'plump', 'same-sex', 'go all the way', 'stout', 'disabled', 'disadvantaged', 'underdeveloped', 'invalid', 'well off', 'economical', 'intoxicated', 'exterminate', 'a certain age', 'between jobs', 'long sleep', 'getting clean', 'downsize', 'expecting', 'lay off', 'laid off', 'to go to heaven', 'accident', 'outlived her usefulness', 'outlived their usefulness', 'got clean', 'late', 'wealthy', 'troubled', 'seeing someone', 'seasoned', 'sleep with', 'slept with', 'sleep around', 'with child', 'weed', 'let them go', 'let us go', 'let her go', 'pass away', 'regime change', 'custodian', 'outlived his usefulness', 'outspoken', 'seeing each other']\n",
      "\n",
      "FALSE NEGATIVES of TOPIC FILTERING: ['expecting', 'seasoned', 'outlived his usefulness']\n"
     ]
    }
   ],
   "source": [
    "THRESHOLD = 1.5\n",
    "score = 0\n",
    "\n",
    "successes = []\n",
    "partial_successes = []\n",
    "failures = []\n",
    "topically_filtered_euphs = []\n",
    "quality_phrase_count = 0\n",
    "filtered = []\n",
    "\n",
    "euph_corpus['quality_phrases'] = \"\"\n",
    "\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    text = euph_corpus.loc[i, 'sentence']\n",
    "    phrases = euph_corpus.loc[i, 'phrases']\n",
    "    euph = euph_corpus.loc[i, 'keyword']\n",
    "    quality_phrases = []\n",
    "    for phrase in phrases:\n",
    "        if (phrase in stopwords):\n",
    "            continue\n",
    "        similarity = sum_similarity(phrase, topic_list)\n",
    "        if (similarity > THRESHOLD and phrase not in quality_phrases):\n",
    "            quality_phrases.append(phrase)\n",
    "        elif (similarity < THRESHOLD and euph == re.sub(r'_', ' ', phrase)):\n",
    "            if euph not in topically_filtered_euphs:\n",
    "                topically_filtered_euphs.append(euph)\n",
    "        else:\n",
    "            filtered.append(phrase)\n",
    "    # add the quality phrases to the column\n",
    "    euph_corpus.at[i, 'quality_phrases'] = quality_phrases\n",
    "    \n",
    "    # now check if the euph in the sentence is retained in the list of quality phrases\n",
    "    quality_phrases = [re.sub(r'_', ' ', p) for p in quality_phrases]\n",
    "    quality_phrase_count += len(quality_phrases)\n",
    "    \n",
    "    if euph in quality_phrases:\n",
    "        score += 1\n",
    "        if euph not in successes:\n",
    "            successes.append(euph)\n",
    "    else:\n",
    "        partial_success = False\n",
    "        for p in quality_phrases: # check if phrase output contains euphemism\n",
    "            if euph in p:\n",
    "                score += 1\n",
    "                if euph not in partial_successes:\n",
    "                    partial_successes.append(euph)\n",
    "                    partial_success = True\n",
    "                    break\n",
    "        if (partial_success == False): \n",
    "            if euph not in failures:\n",
    "                failures.append(euph)\n",
    "\n",
    "            # check failures for a particular phrase\n",
    "            # if (euph == \"ethnic cleansing\"):\n",
    "            #     print(\"TEXT: {}\".format(text))\n",
    "            #     print(\"PHRASES: {}\".format(phrases))\n",
    "            #     print(\"QUALITY PHRASES: {}\".format(quality_phrases))\n",
    "            #     print()\n",
    "\n",
    "print(\"Retained the euphemism in {} out of {} sentences\".format(score, len(euph_corpus)))\n",
    "print(\"{} quality phrases retained overall\".format(quality_phrase_count))\n",
    "print(\"Filtered {} non-keywords out\".format(len(filtered)))\n",
    "print()\n",
    "print(\"EXACT SUCCESSES: {}\".format(successes))\n",
    "print()\n",
    "print(\"PARTIAL SUCCESSES: {}\".format(partial_successes))\n",
    "print()\n",
    "print(\"FAILURES: {}\".format(failures))\n",
    "print()\n",
    "print(\"FALSE NEGATIVES of TOPIC FILTERING: {}\".format(topically_filtered_euphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "194d9de7-d23f-4b43-9f77-26f87fa262ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics: 0.033825889229774475\n",
      "death: -0.02670614793896675\n",
      "kill: 0.3173597753047943\n",
      "crime: 0.2425854206085205\n",
      "drugs: 0.47998520731925964\n",
      "alcohol: 0.5207271575927734\n",
      "fat: 0.4031992256641388\n",
      "old: 0.21074526011943817\n",
      "poor: 0.19883549213409424\n",
      "cheap: 0.2884795069694519\n",
      "sex: 0.20405223965644836\n",
      "sexual: 0.0429307259619236\n",
      "employment: 0.009533492848277092\n",
      "job: 0.17653518915176392\n",
      "disability: -0.009102713316679\n",
      "disabled: -0.020909197628498077\n",
      "accident: 0.035558976233005524\n",
      "pregnant: 0.2644718289375305\n",
      "poop: 0.4726288914680481\n",
      "sickness: 0.24091216921806335\n",
      "race: 0.054482314735651016\n",
      "racial: 0.024806775152683258\n",
      "vomit: 0.47665008902549744\n",
      "SIMILAR TOPICS: ['kill', 'crime', 'drugs', 'alcohol', 'fat', 'cheap', 'pregnant', 'poop', 'sickness', 'vomit']\n",
      "TOTAL SCORE: 4.698305627331138\n"
     ]
    }
   ],
   "source": [
    "# testing - for topic similarity queries on a single phrase\n",
    "test_phrase = 'weed'\n",
    "similar_topics = []\n",
    "score = 0\n",
    "for topic in topic_list:\n",
    "    similarity = model.wv.similarity(test_phrase, topic)\n",
    "    if (similarity > 0.24):\n",
    "        similar_topics.append(topic)\n",
    "    if (similarity > 0):\n",
    "        score += similarity\n",
    "    print('{}: {}'.format(topic, similarity))\n",
    "\n",
    "print('SIMILAR TOPICS: {}'.format(similar_topics))\n",
    "print('TOTAL SCORE: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa0c452-ce5d-428d-9a7e-96a2a34158bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ca372-a68c-40a0-b74c-de85495a7dda",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### roBERTa Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e50b4183-9386-4227-9a77-39af5819c9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "def load_roberta_sentiment():\n",
    "    # Tasks:\n",
    "    # emoji, emotion, hate, irony, offensive, sentiment\n",
    "    # stance/abortion, stance/atheism, stance/climate, stance/feminist, stance/hillary\n",
    "\n",
    "    task='sentiment'\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "    # download label mapping\n",
    "    labels=[]\n",
    "    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "    with urllib.request.urlopen(mapping_link) as f:\n",
    "        html = f.read().decode('utf-8').split(\"\\n\")\n",
    "        csvreader = csv.reader(html, delimiter='\\t')\n",
    "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "    # pretrained\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    model.save_pretrained(MODEL)\n",
    "    tokenizer.save_pretrained(MODEL)\n",
    "    \n",
    "    return labels, model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2d014-f2c2-45d1-86aa-638521641334",
   "metadata": {},
   "source": [
    "#### roBERTa Offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72ec1e38-0915-4a24-8b40-3d4ab7e13a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_roberta_offensive():\n",
    "    task='offensive'\n",
    "    MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "    # download label mapping\n",
    "    labels=[]\n",
    "    mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "    with urllib.request.urlopen(mapping_link) as f:\n",
    "        html = f.read().decode('utf-8').split(\"\\n\")\n",
    "        csvreader = csv.reader(html, delimiter='\\t')\n",
    "    labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "    # PT\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "    model.save_pretrained(MODEL)\n",
    "    tokenizer.save_pretrained(MODEL)\n",
    "    \n",
    "    return labels, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4893e394-9a6b-49ae-8d40-8cf8a6253ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(s, labels, model, tokenizer):\n",
    "    encoded_input = tokenizer(s, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    # ranking = np.argsort(scores)\n",
    "    # ranking = ranking[::-1]\n",
    "    # for i in range(scores.shape[0]):\n",
    "    #     l = labels[ranking[i]]\n",
    "    #     s = scores[ranking[i]]\n",
    "        # print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "    return scores\n",
    "\n",
    "def get_offensive(s, labels, model, tokenizer):\n",
    "    encoded_input = tokenizer(s, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    # ranking = np.argsort(scores)\n",
    "    # ranking = ranking[::-1]\n",
    "    # for i in range(0, 2):\n",
    "    #     l = labels[ranking[i]]\n",
    "    #     s = scores[ranking[i]]\n",
    "        # print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad5eaa9-5ad6-4619-984a-19fedb07e4ce",
   "metadata": {},
   "source": [
    "#### Run sentiment/offensive analysis on euph corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "659d594a-1069-4d20-8106-d6f6f592ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "needs functions load_roberta_sentiment(), load_roberta_offensive(), get_sentiment() and get_offensive()\n",
    "'''\n",
    "def get_top_euph_candidates(text, phrases, num_paraphrases, wv_model, sentiment_pack, offensive_pack, show_stats=False):\n",
    "    \n",
    "    sentiment_labels, sentiment_model, sentiment_tokenizer = sentiment_pack[0], sentiment_pack[1], sentiment_pack[2]\n",
    "    offensive_labels, offensive_model, offensive_tokenizer = offensive_pack[0], offensive_pack[1], offensive_pack[2]\n",
    "    \n",
    "    orig_scores = list(get_sentiment(text, sentiment_labels, sentiment_model, sentiment_tokenizer))\n",
    "    orig_scores = orig_scores + list(get_offensive(text, offensive_labels, offensive_model, offensive_tokenizer))\n",
    "    if show_stats == True: print('SENTIMENT OF ORIGINAL SENTENCE: {}'.format(orig_scores))\n",
    "    phrase_scores = []\n",
    "\n",
    "    for q in phrases:\n",
    "        paraphrases = []\n",
    "        if show_stats == True: print('\\n'+q)\n",
    "        paraphrases = wv_model.wv.most_similar(q, topn = num_paraphrases) # can swap out\n",
    "        \n",
    "        # print(paraphrases)\n",
    "        \n",
    "        # various sentiment statistics\n",
    "        sentiment_shift = [0, 0, 0, 0, 0]\n",
    "        max_inc = [0, 0, 0, 0, 0]\n",
    "        max_inc_para = [\"\", \"\", \"\", \"\", \"\"]\n",
    "        tot_neg_inc = 0\n",
    "        tot_neu_inc = 0\n",
    "        tot_off_inc = 0\n",
    "        tot_noff_inc = 0\n",
    "        \n",
    "        for p in paraphrases:\n",
    "            p_string = re.sub(r'_', ' ', p[0]) # the underscores are removed for sentiment computation - experiment?\n",
    "            q_string = re.sub(r'_', ' ', q)\n",
    "            # replacement\n",
    "            pattern = re.compile(r'\\b'+q_string+r'\\b', re.IGNORECASE)\n",
    "            new_sentence = pattern.sub(p_string, text)\n",
    "            # at this point, we could check the integrity of the paraphrase\n",
    "\n",
    "            # get the sentiment/offensive scores for this paraphrase\n",
    "            scores = list(get_sentiment(new_sentence, sentiment_labels, sentiment_model, sentiment_tokenizer))\n",
    "            scores = scores + list(get_offensive(new_sentence, offensive_labels, offensive_model, offensive_tokenizer))\n",
    "\n",
    "            # update the quality phrase's sentiment statistics with the sentiment shifts from this paraphrase\n",
    "            shifts = [0, 0, 0, 0, 0]\n",
    "            for i in range(0, len(scores)):\n",
    "                shifts[i] = scores[i] - orig_scores[i]\n",
    "                sentiment_shift[i] += shifts[i]\n",
    "                if (shifts[i] > max_inc[i]):\n",
    "                    max_inc[i] = shifts[i]\n",
    "                    max_inc_para[i] = p_string\n",
    "\n",
    "            # update the relevant scores for detection\n",
    "            if (shifts[0] > 0):\n",
    "                tot_neg_inc += shifts[0]\n",
    "            if (shifts[1] > 0):\n",
    "                tot_neu_inc += shifts[1]\n",
    "            if (shifts[3] > 0):\n",
    "                tot_noff_inc += shifts[3]\n",
    "            if (shifts[4] > 0):\n",
    "                tot_off_inc += shifts[4]\n",
    "        \n",
    "        for val in sentiment_shift:\n",
    "            val /= num_paraphrases\n",
    "        if (show_stats == True):\n",
    "            print(\"AVERAGE SENTIMENT SHIFTS: {}\".format(sentiment_shift))\n",
    "            print(\"MAX INCREASE FROM A PHRASE: {}\".format(max_inc))\n",
    "            print(\"PHRASES THAT CAUSED EACH ^: {}\".format(max_inc_para))\n",
    "            print(\"TOTAL NEGATIVE INCREASE: {}\".format(tot_neg_inc))\n",
    "            print(\"TOTAL NEUTRAL INCREASE: {}\".format(tot_neu_inc))\n",
    "            print(\"TOTAL NEUTRAL INCREASE: {}\".format(tot_noff_inc))\n",
    "            print(\"TOTAL OFFENSIVE INCREASE: {}\".format(tot_off_inc))\n",
    "\n",
    "        phrase_scores.append((q_string, tot_neg_inc + tot_neu_inc + 2*(tot_noff_inc + tot_off_inc)))\n",
    "\n",
    "    phrase_scores = list(sorted(phrase_scores, key=lambda x: x[1], reverse=True))\n",
    "    return phrase_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45b60dec-e610-437c-833d-8113f0df9a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the models\n",
    "sentiment_labels, sentiment_model, sentiment_tokenizer = load_roberta_sentiment()\n",
    "offensive_labels, offensive_model, offensive_tokenizer = load_roberta_offensive()\n",
    "\n",
    "sentiment_pack = [sentiment_labels, sentiment_model, sentiment_tokenizer]\n",
    "offensive_pack = [offensive_labels, offensive_model, offensive_tokenizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fe47ae8-342f-4071-a9d0-d490a6afbc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "num_paraphrases = 25\n",
    "score = 0\n",
    "k = 2\n",
    "euph_corpus['candidates'] = \"\"\n",
    "euph_corpus['top_2'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e8202d5-38d8-48fa-941d-92906b67e0b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 91/1965 [19:01<7:03:16, 13.55s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 177/1965 [33:56<5:05:58, 10.27s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 256/1965 [49:13<4:01:20,  8.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 342/1965 [1:03:56<3:58:55,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 434/1965 [1:25:59<3:59:21,  9.38s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 549/1965 [1:47:48<3:17:21,  8.36s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 635/1965 [2:07:39<3:26:41,  9.32s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 742/1965 [2:27:35<5:36:18, 16.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 858/1965 [2:44:50<2:11:04,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 948/1965 [2:58:13<1:51:16,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 1039/1965 [3:14:08<3:55:47, 15.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 1154/1965 [3:34:32<1:39:07,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 1289/1965 [3:53:12<1:42:33,  9.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 1382/1965 [4:12:09<1:46:22, 10.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euphemism detected in 693 out of 1965 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i, row in tqdm(euph_corpus.iterrows(), total=euph_corpus.shape[0]):\n",
    "# uncomment below if resuming from checkpoint\n",
    "#     if (0 < i < 600):\n",
    "#         continue\n",
    "    phrases = euph_corpus.loc[i, 'quality_phrases']\n",
    "    \n",
    "    # Converting string to list IF READING FROM CSV as checkpoint\n",
    "    # phrases = ast.literal_eval(phrases)\n",
    "    \n",
    "    text = euph_corpus.loc[i, 'sentence']\n",
    "    euph = euph_corpus.loc[i, 'keyword']\n",
    "    \n",
    "    top_candidates = get_top_euph_candidates(text, phrases, num_paraphrases, model, \n",
    "                                             sentiment_pack, offensive_pack, show_stats=False)\n",
    "#     print(top_candidates)\n",
    "#     print()\n",
    "    euph_corpus.at[i, 'candidates'] = top_candidates\n",
    "    \n",
    "    # check the top k candidates - this code could use cleaning up\n",
    "    for x in range(0, k):\n",
    "        if (len(top_candidates) == 0):\n",
    "            break\n",
    "        if (len(top_candidates) == 1):\n",
    "            candidate = top_candidates[0][0]\n",
    "            if euph in candidate:\n",
    "                score += 1\n",
    "                if (score % 50 == 0):\n",
    "                    print(score)\n",
    "                euph_corpus.loc[i, 'top_2'] = 1\n",
    "            break\n",
    "        candidate = top_candidates[x][0]\n",
    "        if euph in candidate:\n",
    "            score += 1\n",
    "            if (score % 50 == 0):\n",
    "                print(score)\n",
    "            euph_corpus.loc[i, 'top_2'] = 1\n",
    "            break\n",
    "\n",
    "    if (i == 1382):\n",
    "        break\n",
    "print(\"Euphemism detected in {} out of {} sentences\".format(score, len(euph_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9fce9-5a69-4a5a-aa2a-d600e7718903",
   "metadata": {},
   "source": [
    "#### Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f985515b-beed-4369-a6b6-7f3f3a5e984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "euph_corpus.to_csv('results_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38148a65-22d2-482d-8514-e31af5e67518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "euph_corpus = pd.read_csv('results_8.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2188bcb-fe52-459f-b3c0-154277887c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443\n",
      "250\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "# TODO: compute 1st place 2nd place \n",
    "num_first_place = 0\n",
    "num_second_place = 0\n",
    "num_third_place = 0\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    if (i > 1382):\n",
    "        continue\n",
    "    top_2 = euph_corpus.loc[i, 'top_2']\n",
    "    keyword = euph_corpus.loc[i, 'keyword']\n",
    "    candidates = euph_corpus.loc[i, 'candidates']\n",
    "    # Converting string to list\n",
    "    candidates = ast.literal_eval(candidates)\n",
    "    if (top_2 == 1):\n",
    "        if (keyword in candidates[0][0]):\n",
    "            num_first_place += 1\n",
    "        elif (keyword in candidates[1][0]):\n",
    "            num_second_place += 1\n",
    "    elif (len(candidates) > 2):\n",
    "        if (keyword in candidates[2][0]):\n",
    "            num_third_place += 1\n",
    "\n",
    "print(num_first_place)\n",
    "print(num_second_place)\n",
    "print(num_third_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dabeae64-67c2-4436-8f50-fd3d15c96213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1251\n",
      "31348\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "count = 0\n",
    "tot_p = 0\n",
    "# denote rows where keyword was present in REGULAR phrases\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    if (euph_corpus.loc[i, \"is_euph\"] == 0):\n",
    "        continue\n",
    "    phrases = euph_corpus.loc[i, \"phrases\"]\n",
    "    # Converting string to list\n",
    "    phrases = ast.literal_eval(phrases)\n",
    "    tot_p += len(phrases)\n",
    "    keyword = euph_corpus.loc[i, 'keyword']\n",
    "    for p in phrases:\n",
    "        p_string = re.sub(r'_', ' ', p)\n",
    "        if keyword in p_string:\n",
    "            #euph_corpus.loc[i, 'keyword_present'] = 1\n",
    "            count += 1\n",
    "            break\n",
    "            \n",
    "print(count)\n",
    "print(tot_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da585b8f-319f-4830-bbff-e131a8376572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1199\n",
      "10497\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "count = 0\n",
    "tot_q = 0\n",
    "# denote rows where keyword was present in quality phrases\n",
    "euph_corpus['keyword_present'] = 0\n",
    "for i, row in euph_corpus.iterrows():\n",
    "    if (euph_corpus.loc[i, \"is_euph\"] == 0):\n",
    "        continue\n",
    "    quality_phrases = euph_corpus.loc[i, \"quality_phrases\"]\n",
    "    # Converting string to list\n",
    "    quality_phrases = ast.literal_eval(quality_phrases)\n",
    "    tot_q += len(quality_phrases)\n",
    "    keyword = euph_corpus.loc[i, 'keyword']\n",
    "    for q in quality_phrases:\n",
    "        q_string = re.sub(r'_', ' ', q)\n",
    "        if keyword in q_string:\n",
    "            euph_corpus.loc[i, 'keyword_present'] = 1\n",
    "            count += 1\n",
    "            break\n",
    "\n",
    "print(count)\n",
    "print(tot_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bed96f10-9af5-4b53-9535-67b56a74c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "euph_corpus.to_csv('results_8.2.1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
